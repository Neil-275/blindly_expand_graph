{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ccacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942a5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09bb13",
   "metadata": {},
   "source": [
    "### Khởi tạo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192de6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/id2ent.pkl\", \"rb\") as f:\n",
    "#     id2ent = pkl.load(f)\n",
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/id2rel.pkl\", \"rb\") as f:\n",
    "#     id2rel = pkl.load(f)\n",
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/FB15k_mid2name.txt\", \"r\") as f:\n",
    "#     ent2name = {}\n",
    "#     for line in f:\n",
    "#         mid, name = line.strip().split(\"\\t\")\n",
    "#         ent2name[mid] = name\n",
    "\n",
    "with open(\"knowledge_graph/queries/train_2c_id.pkl\", \"rb\") as f:\n",
    "    query_2_hop = pkl.load(f)\n",
    "with open(\"knowledge_graph/queries/train_1c_id.pkl\", \"rb\") as f:\n",
    "    query_1_hop = pkl.load(f)\n",
    "with open(\"knowledge_graph/queries/train_3c_id.pkl\", \"rb\") as f:\n",
    "    query_3_hop = pkl.load(f)\n",
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/train-answers.pkl\", \"rb\") as f:\n",
    "#     answer_train = pkl.load(f)\n",
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/valid-easy-answers.pkl\", \"rb\") as f:\n",
    "#     answer_valid_easy = pkl.load(f)\n",
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/test-hard-answers.pkl\", \"rb\") as f:\n",
    "#     answer_test_hard = pkl.load(f)\n",
    "# with open(\"knowledge_graph/KG_data/FB15k-237-betae/test-easy-answers.pkl\", \"rb\") as f:\n",
    "#     answer_test_easy = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0fc09",
   "metadata": {},
   "source": [
    "### Tạo embedding cho các relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc12039",
   "metadata": {},
   "source": [
    "Nhận xét:  \n",
    "+ Direct và inverse relation thường có độ khớp gần như nhau.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ed0f0",
   "metadata": {},
   "source": [
    "Original query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53cb2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which awards has David Copperfield received for his performances in films?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104e2a7",
   "metadata": {},
   "source": [
    "#### Query split:  \n",
    "Do query thể có nhiều phần và cần phải qua nhiều hop mới đạt được kết quả. Do đó, ta có thể đưa query về thành các sub-queries và xử lí từng đoạn.  \n",
    "+ Splitting queries: Ta có thể đưa LLM làm (PoG có bước này)\n",
    "+ Compose results: \n",
    "    + Sử dụng mean\n",
    "    + Sử dụng reciprocal rank\n",
    "    + "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f53380",
   "metadata": {},
   "source": [
    "### Inspect queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f22b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_type': ('e', ('r', 'r')),\n",
       " 'raw_query': (4582, (133, 17)),\n",
       " 'named_query': ('Franklin',\n",
       "  ('-/people/person/places_lived./people/place_lived/location',\n",
       "   '-/award/award_nominee/award_nominations./award/award_nomination/award_nominee')),\n",
       " 'transformed_query': ['Who are the award nominees that lived in places associated with Franklin?',\n",
       "  'Which award nominees have resided in locations tied to Franklin?',\n",
       "  'Can you tell me the names of award nominees who lived in locations related to Franklin?'],\n",
       " 'answers_id': [2403, 1221, 137, 1589, 4093],\n",
       " 'answers': ['Vince_Gill',\n",
       "  'Albert_Lee',\n",
       "  'Keith_Urban',\n",
       "  'Carrie_Underwood',\n",
       "  'John_Travolta']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2_hop[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb34dd8",
   "metadata": {},
   "source": [
    "### Test pipeline extract subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45f49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expand_subgraph import ExpandSubgraph\n",
    "from utils import  extract_numbers, extract_strings,\\\n",
    "                    extract_notations\n",
    "\n",
    "import argparse\n",
    "from load_data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c4817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> removing 1-hop links...\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    data_path = 'knowledge_graph/KG_data/FB15k-237-betae'\n",
    "    seed = 1234\n",
    "    k = 6 # beams\n",
    "    depth = 8 # max depth of subgraph\n",
    "    gpu = 0\n",
    "    fact_ratio = 0.75\n",
    "    val_num = -1 # how many triples are used as the validate set\n",
    "    epoch = 200\n",
    "    layer = 6\n",
    "    batchsize = 16\n",
    "    cpu = 1\n",
    "    weight = ''\n",
    "    add_manual_edges = False\n",
    "    remove_1hop_edges = True\n",
    "    only_eval = False\n",
    "    not_shuffle_train = False\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "args = Config()\n",
    "\n",
    "loader = DataLoader(args, mode='train')\n",
    "# val_loader = DataLoader(args, mode='valid')\n",
    "# test_loader = DataLoader(args, mode='test')\n",
    "\n",
    "train_graph = loader.train_graph\n",
    "train_graph_homo = list(set([(h,t) for (h,r,t) in train_graph]))\n",
    "# test_data = np.concatenate([np.array(test_data, dtype=np.int32), loader.idd_data], 0, dtype=np.int32)\n",
    "# train_graph = np.array(train_graph, dtype=np.int32)\n",
    "args.n_ent = loader.n_ent\n",
    "args.n_rel = loader.n_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5801874",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subgraph = defaultdict(list)\n",
    "def print_statistics(stats, label):\n",
    "    print(f\"{label} Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        if key in [\"mean\", \"std_dev\"]:\n",
    "            print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "def calculate_overall_score(query_set, train_sampler, method=\"default\", label=None):\n",
    "    overall_score = 0\n",
    "    for query in query_set:\n",
    "        if method == \"default\":\n",
    "            topk_node, _, subgraph = train_sampler.sampleSubgraph(query)\n",
    "        elif method == \"bfs\":\n",
    "            topk_node, _, subgraph = train_sampler.sampleSubgraphBFS(query)\n",
    "        s = 0\n",
    "        all_subgraph[label].append({\n",
    "            \"query\": query,\n",
    "            \"subgraph\": subgraph\n",
    "        })\n",
    "        answers = set(query[\"answers_id\"])\n",
    "        topk_node_set = set(topk_node)\n",
    "        precision = len(answers & topk_node_set) / len(answers) if len(answers) > 0 else 0\n",
    "        overall_score += precision\n",
    "    overall_score /= len(query_set)\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9123f",
   "metadata": {},
   "source": [
    "#### P2.s1 approach: Measuring performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "076d6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args)\n",
    "\n",
    "query_1_hop_stat = []\n",
    "query_2_hop_stat = []\n",
    "query_3_hop_stat = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat.append(calculate_overall_score(query_1_hop[:30], train_sampler, label = \"p2s2\"))\n",
    "    query_2_hop_stat.append(calculate_overall_score(query_2_hop[:30], train_sampler, label = \"p2s2\"))\n",
    "    query_3_hop_stat.append(calculate_overall_score(query_3_hop[:30], train_sampler, label = \"p2s2\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fb3acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.7939\n",
      "  std_dev: 0.0058\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.3045\n",
      "  std_dev: 0.0031\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.1924\n",
      "  std_dev: 0.0049\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_statistics\n",
    "\n",
    "\n",
    "stats_1_hop = calculate_statistics(query_1_hop_stat)\n",
    "print_statistics(stats_1_hop, \"1-Hop Query\")\n",
    "stats_2_hop = calculate_statistics(query_2_hop_stat)\n",
    "print_statistics(stats_2_hop, \"2-Hop Query\")\n",
    "stats_3_hop = calculate_statistics(query_3_hop_stat)\n",
    "print_statistics(stats_3_hop, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290dffe3",
   "metadata": {},
   "source": [
    "#### Vanila BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fec64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m random.shuffle(query_2_hop)\n\u001b[32m     17\u001b[39m random.shuffle(query_3_hop)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m query_1_hop_stat_bfs.append(\u001b[43mcalculate_overall_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_1_hop\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbfs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbfs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m query_2_hop_stat_bfs.append(calculate_overall_score(query_2_hop[:\u001b[32m50\u001b[39m], train_sampler, \u001b[33m\"\u001b[39m\u001b[33mbfs\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mbfs\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     20\u001b[39m query_3_hop_stat_bfs.append(calculate_overall_score(query_3_hop[:\u001b[32m50\u001b[39m], train_sampler, \u001b[33m\"\u001b[39m\u001b[33mbfs\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mbfs\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcalculate_overall_score\u001b[39m\u001b[34m(query_set, train_sampler, method, label)\u001b[39m\n\u001b[32m      6\u001b[39m     topk_node, _, subgraph = train_sampler.sampleSubgraph(query)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mbfs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     topk_node, _, subgraph = \u001b[43mtrain_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msampleSubgraphBFS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m s = \u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m all_subgraph[label].append({\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msubgraph\u001b[39m\u001b[33m\"\u001b[39m: subgraph\n\u001b[32m     13\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\code\\research\\messing_around\\prototype\\expand_subgraph.py:263\u001b[39m, in \u001b[36mExpandSubgraph.sampleSubgraphBFS\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    262\u001b[39m triplets = [(current_entity, rel_id, tail_id) \u001b[38;5;28;01mfor\u001b[39;00m rel_id, tail_id \u001b[38;5;129;01min\u001b[39;00m adjacency_list[current_entity]]\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m triplets = np.array([triplet \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets \u001b[38;5;28;01mif\u001b[39;00m (triplet[\u001b[32m0\u001b[39m], triplet[\u001b[32m1\u001b[39m], triplet[\u001b[32m2\u001b[39m]) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.visited \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id2name(triplet[\u001b[32m2\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mUnName_Entity\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mself\u001b[39m.visited.add((triplet[\u001b[32m0\u001b[39m], triplet[\u001b[32m1\u001b[39m], triplet[\u001b[32m2\u001b[39m]))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args)\n",
    "\n",
    "query_1_hop_stat_bfs = []\n",
    "query_2_hop_stat_bfs = []\n",
    "query_3_hop_stat_bfs = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat_bfs.append(calculate_overall_score(query_1_hop[:50], train_sampler, \"bfs\", label=\"bfs\"))\n",
    "    query_2_hop_stat_bfs.append(calculate_overall_score(query_2_hop[:50], train_sampler, \"bfs\", label=\"bfs\"))\n",
    "    query_3_hop_stat_bfs.append(calculate_overall_score(query_3_hop[:50], train_sampler, \"bfs\", label=\"bfs\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.7970\n",
      "  std_dev: 0.0118\n",
      "1-Hop Query Statistics:\n",
      "  mean: 0.7970\n",
      "  std_dev: 0.0118\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.4025\n",
      "  std_dev: 0.0118\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.4431\n",
      "  std_dev: 0.0123\n"
     ]
    }
   ],
   "source": [
    "stats_1_hop_bfs = calculate_statistics(query_1_hop_stat_bfs)\n",
    "print_statistics(stats_1_hop_bfs, \"1-Hop Query\")\n",
    "stats_2_hop_bfs = calculate_statistics(query_2_hop_stat_bfs)\n",
    "print_statistics(stats_2_hop_bfs, \"2-Hop Query\")\n",
    "stats_3_hop_bfs = calculate_statistics(query_3_hop_stat_bfs)\n",
    "print_statistics(stats_3_hop_bfs, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055757e",
   "metadata": {},
   "source": [
    "### Sub-objective approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d8fab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai error, retry: 1 validation error for SubobjectiveOutput\n",
      "  Invalid JSON: EOF while parsing a string at line 1 column 169 [type=json_invalid, input_value='{\"res\":[\"Identify the aw... the awards he has been', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "epoch 0\n",
      "epoch 1\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args, use_sub_objectives_a=True)\n",
    "\n",
    "query_1_hop_stat_a = []\n",
    "query_2_hop_stat_a = []\n",
    "query_3_hop_stat_a = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat_a.append(calculate_overall_score(query_1_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_2_hop_stat_a.append(calculate_overall_score(query_2_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_3_hop_stat_a.append(calculate_overall_score(query_3_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8025a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.2864\n",
      "  std_dev: 0.0002\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.0949\n",
      "  std_dev: 0.0367\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.0416\n",
      "  std_dev: 0.0312\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_statistics\n",
    "\n",
    "\n",
    "stats_1_hop_a = calculate_statistics(query_1_hop_stat_a)\n",
    "print_statistics(stats_1_hop_a, \"1-Hop Query\")\n",
    "stats_2_hop_a = calculate_statistics(query_2_hop_stat_a)\n",
    "print_statistics(stats_2_hop_a, \"2-Hop Query\")\n",
    "stats_3_hop_a = calculate_statistics(query_3_hop_stat_a)\n",
    "print_statistics(stats_3_hop_a, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c772d5",
   "metadata": {},
   "source": [
    "### Sub-objective approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8d9d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> removing 1-hop links...\n",
      "==> done\n",
      "epoch 0\n",
      "==> removing 1-hop links...\n",
      "==> done\n",
      "epoch 1\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args, use_sub_objectives_b=True)\n",
    "\n",
    "query_1_hop_stat_b = []\n",
    "query_2_hop_stat_b = []\n",
    "query_3_hop_stat_b = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat_b.append(calculate_overall_score(query_1_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_2_hop_stat_b.append(calculate_overall_score(query_2_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_3_hop_stat_b.append(calculate_overall_score(query_3_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44297ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.2926\n",
      "  std_dev: 0.0399\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.1010\n",
      "  std_dev: 0.0069\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.0475\n",
      "  std_dev: 0.0114\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_statistics\n",
    "\n",
    "\n",
    "stats_1_hop_b = calculate_statistics(query_1_hop_stat_b)\n",
    "print_statistics(stats_1_hop_b, \"1-Hop Query\")\n",
    "stats_2_hop_b = calculate_statistics(query_2_hop_stat_b)\n",
    "print_statistics(stats_2_hop_b, \"2-Hop Query\")\n",
    "stats_3_hop_b = calculate_statistics(query_3_hop_stat_b)\n",
    "print_statistics(stats_3_hop_b, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadfaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad16362",
   "metadata": {},
   "source": [
    "### Analyzing the size of retrieved subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66e049ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.31111111111111"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_size = 0\n",
    "for i in range(len(all_subgraph[\"p2s2\"])):\n",
    "    subgraph = all_subgraph[\"p2s2\"][i][\"subgraph\"]\n",
    "    # print(subgraph)\n",
    "    # print(subgraph[:,[0,2]].flatten())\n",
    "    number_of_nodes = np.unique(subgraph[:,[0,2]].flatten()).shape[0]\n",
    "    # print(number_of_nodes)\n",
    "    avg_size += number_of_nodes\n",
    "    # break\n",
    "avg_size /= len(all_subgraph[\"p2s2\"])\n",
    "avg_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85371e2",
   "metadata": {},
   "source": [
    "### output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47298b81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats_1_hop_bfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m results = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmine\u001b[39m\u001b[33m\"\u001b[39m:{\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m1-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_1_hop,\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_2_hop,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m3-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_3_hop\n\u001b[32m      7\u001b[39m     },\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbfs\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m1-hop\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mstats_1_hop_bfs\u001b[49m,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_2_hop_bfs,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m3-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_3_hop_bfs\n\u001b[32m     12\u001b[39m     },\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msubobjective\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m1-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_1_hop_a,\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_2_hop_a,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m3-hop\u001b[39m\u001b[33m\"\u001b[39m: stats_3_hop_a\n\u001b[32m     17\u001b[39m     }\n\u001b[32m     18\u001b[39m }\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mresults/expand_subgraph/expand_subgraph_results1.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     20\u001b[39m     json.dump(results, f, indent=\u001b[32m4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'stats_1_hop_bfs' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "results = {\n",
    "    \"mine\":{\n",
    "        \"1-hop\": stats_1_hop,\n",
    "        \"2-hop\": stats_2_hop,\n",
    "        \"3-hop\": stats_3_hop\n",
    "    },\n",
    "    \"bfs\": {\n",
    "        \"1-hop\": stats_1_hop_bfs,\n",
    "        \"2-hop\": stats_2_hop_bfs,\n",
    "        \"3-hop\": stats_3_hop_bfs\n",
    "    },\n",
    "    \"subobjective\": {\n",
    "        \"1-hop\": stats_1_hop_a,\n",
    "        \"2-hop\": stats_2_hop_a,\n",
    "        \"3-hop\": stats_3_hop_a\n",
    "    }\n",
    "}\n",
    "with open(\"results/expand_subgraph/expand_subgraph_results1.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d2363",
   "metadata": {},
   "source": [
    "#### Evaluate coverage\n",
    "\n",
    "**Answer Files Structure:**\n",
    "+ `valid-easy-answers/test-easy-answers.pkl`: defaultdict(set)\n",
    "  - Each key represents a query\n",
    "  - Value represents the answers obtained in the training graph (edges in `train.txt`) / valid graph (edges in `train.txt` + `valid.txt`)\n",
    "\n",
    "+ `valid-hard-answers/test-hard-answers.pkl`: defaultdict(set)  \n",
    "  - Each key represents a query\n",
    "  - Value represents the additional answers obtained in the validation graph (edges in `train.txt` + `valid.txt`) / test graph (edges in `train.txt` + `valid.txt` + `test.txt`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
