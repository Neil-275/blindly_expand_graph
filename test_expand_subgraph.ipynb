{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ccacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942a5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e265ce30",
   "metadata": {},
   "source": [
    "### Task:\n",
    "Notebook này là để thử nghiệm các phương pháp trích xuất subgraph. Cụ thể:   \n",
    "- Depth và width được giữ nguyên cho các phương pháp  \n",
    "- Các phương pháp được chạy bao gồm:  \n",
    "    + BFS  \n",
    "    + Cho LLM repharase nhiều phiên bản query tính score từng cái rồi gộp lại  \n",
    "    + Cho LLM phân ra thành các sub objective theo prompt của PoG, score từng cái rồi gộp lại\n",
    "    + Cho LLM phân ra nhỏ query theo các relation trong query, score từng cái rồi gộp lại."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09bb13",
   "metadata": {},
   "source": [
    "### Khởi tạo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192de6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"knowledge_graph/KG_data/FB15k-237-betae/id2ent.pkl\", \"rb\") as f:\n",
    "    id2ent = pkl.load(f)\n",
    "with open(\"knowledge_graph/KG_data/FB15k-237-betae/id2rel.pkl\", \"rb\") as f:\n",
    "    id2rel = pkl.load(f)\n",
    "with open(\"knowledge_graph/KG_data/FB15k-237-betae/FB15k_mid2name.txt\", \"r\") as f:\n",
    "    ent2name = {}\n",
    "    for line in f:\n",
    "        mid, name = line.strip().split(\"\\t\")\n",
    "        ent2name[mid] = name\n",
    "\n",
    "with open(\"knowledge_graph/queries/train_2c_id.pkl\", \"rb\") as f:\n",
    "    query_2_hop = pkl.load(f)\n",
    "with open(\"knowledge_graph/queries/train_1c_id.pkl\", \"rb\") as f:\n",
    "    query_1_hop = pkl.load(f)\n",
    "with open(\"knowledge_graph/queries/train_3c_id.pkl\", \"rb\") as f:\n",
    "    query_3_hop = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f53380",
   "metadata": {},
   "source": [
    "### Inspect queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f22b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_type': ('e', ('r', 'r')),\n",
       " 'raw_query': (4582, (133, 17)),\n",
       " 'named_query': ('Franklin',\n",
       "  ('-/people/person/places_lived./people/place_lived/location',\n",
       "   '-/award/award_nominee/award_nominations./award/award_nomination/award_nominee')),\n",
       " 'transformed_query': ['Who are the award nominees that lived in places associated with Franklin?',\n",
       "  'Which award nominees have resided in locations tied to Franklin?',\n",
       "  'Can you tell me the names of award nominees who lived in locations related to Franklin?'],\n",
       " 'answers_id': [2403, 1221, 137, 1589, 4093],\n",
       " 'answers': ['Vince_Gill',\n",
       "  'Albert_Lee',\n",
       "  'Keith_Urban',\n",
       "  'Carrie_Underwood',\n",
       "  'John_Travolta']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2_hop[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb34dd8",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e45f49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expand_subgraph import ExpandSubgraph\n",
    "from utils import  extract_numbers, extract_strings,\\\n",
    "                    extract_notations\n",
    "\n",
    "import argparse\n",
    "from load_data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> removing 1-hop links...\n",
      "==> done\n",
      "==> removing 1-hop links...\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    data_path = 'knowledge_graph/KG_data/FB15k-237-betae'\n",
    "    seed = 1234\n",
    "    k = 9 # beams\n",
    "    depth = 8 # max depth of subgraph\n",
    "    gpu = 0\n",
    "    fact_ratio = 0.75\n",
    "    val_num = -1 # how many triples are used as the validate set\n",
    "    epoch = 200\n",
    "    layer = 6\n",
    "    batchsize = 16\n",
    "    cpu = 1\n",
    "    weight = ''\n",
    "    add_manual_edges = False\n",
    "    remove_1hop_edges = True\n",
    "    only_eval = False\n",
    "    not_shuffle_train = False\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "args = Config()\n",
    "\n",
    "loader = DataLoader(args, mode='train')\n",
    "loader.shuffle_train()\n",
    "# val_loader = DataLoader(args, mode='valid')\n",
    "# test_loader = DataLoader(args, mode='test')\n",
    "\n",
    "train_graph = loader.train_graph\n",
    "train_graph_homo = list(set([(h,t) for (h,r,t) in train_graph]))\n",
    "# test_data = np.concatenate([np.array(test_data, dtype=np.int32), loader.idd_data], 0, dtype=np.int32)\n",
    "# train_graph = np.array(train_graph, dtype=np.int32)\n",
    "args.n_ent = loader.n_ent\n",
    "args.n_rel = loader.n_rel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84292451",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5801874",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subgraph = defaultdict(list)\n",
    "def print_statistics(stats, label):\n",
    "    print(f\"{label} Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        if key in [\"mean\", \"std_dev\"]:\n",
    "            print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "def calculate_overall_score(query_set, train_sampler, method=\"default\", label=None):\n",
    "    overall_score = 0\n",
    "    for query in query_set:\n",
    "        if method == \"default\":\n",
    "            topk_node, _, subgraph = train_sampler.sampleSubgraph(query)\n",
    "        elif method == \"bfs\":\n",
    "            topk_node, _, subgraph = train_sampler.sampleSubgraphBFS(query)\n",
    "        s = 0\n",
    "        all_subgraph[label].append({\n",
    "            \"query\": query,\n",
    "            \"subgraph\": subgraph\n",
    "        })\n",
    "        answers = set(query[\"answers_id\"])\n",
    "        topk_node_set = set(topk_node)\n",
    "        precision = len(answers & topk_node_set) / len(answers) if len(answers) > 0 else 0\n",
    "        overall_score += precision\n",
    "    overall_score /= len(query_set)\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9123f",
   "metadata": {},
   "source": [
    "#### P2.s0 approach: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> removing 1-hop links...\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args)\n",
    "\n",
    "query_1_hop_stat = []\n",
    "query_2_hop_stat = []\n",
    "query_3_hop_stat = []\n",
    "\n",
    "for _ in range(1):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat.append(calculate_overall_score(query_1_hop[:100], train_sampler, label = \"p2s2\"))\n",
    "    query_2_hop_stat.append(calculate_overall_score(query_2_hop[:100], train_sampler, label = \"p2s2\"))\n",
    "    query_3_hop_stat.append(calculate_overall_score(query_3_hop[:100], train_sampler, label = \"p2s2\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3acf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.7939\n",
      "  std_dev: 0.0058\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.3045\n",
      "  std_dev: 0.0031\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.1924\n",
      "  std_dev: 0.0049\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_statistics\n",
    "\n",
    "\n",
    "stats_1_hop = calculate_statistics(query_1_hop_stat)\n",
    "print_statistics(stats_1_hop, \"1-Hop Query\")\n",
    "stats_2_hop = calculate_statistics(query_2_hop_stat)\n",
    "print_statistics(stats_2_hop, \"2-Hop Query\")\n",
    "stats_3_hop = calculate_statistics(query_3_hop_stat)\n",
    "print_statistics(stats_3_hop, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290dffe3",
   "metadata": {},
   "source": [
    "#### Vanila BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args)\n",
    "\n",
    "query_1_hop_stat_bfs = []\n",
    "query_2_hop_stat_bfs = []\n",
    "query_3_hop_stat_bfs = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat_bfs.append(calculate_overall_score(query_1_hop[:50], train_sampler, \"bfs\", label=\"bfs\"))\n",
    "    query_2_hop_stat_bfs.append(calculate_overall_score(query_2_hop[:50], train_sampler, \"bfs\", label=\"bfs\"))\n",
    "    query_3_hop_stat_bfs.append(calculate_overall_score(query_3_hop[:50], train_sampler, \"bfs\", label=\"bfs\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.7970\n",
      "  std_dev: 0.0118\n",
      "1-Hop Query Statistics:\n",
      "  mean: 0.7970\n",
      "  std_dev: 0.0118\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.4025\n",
      "  std_dev: 0.0118\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.4431\n",
      "  std_dev: 0.0123\n"
     ]
    }
   ],
   "source": [
    "stats_1_hop_bfs = calculate_statistics(query_1_hop_stat_bfs)\n",
    "print_statistics(stats_1_hop_bfs, \"1-Hop Query\")\n",
    "stats_2_hop_bfs = calculate_statistics(query_2_hop_stat_bfs)\n",
    "print_statistics(stats_2_hop_bfs, \"2-Hop Query\")\n",
    "stats_3_hop_bfs = calculate_statistics(query_3_hop_stat_bfs)\n",
    "print_statistics(stats_3_hop_bfs, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4b20150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args)\n",
    "for _ in range(3):\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    calculate_overall_score(query_1_hop[0:1], train_sampler, \"bfs\", label = \"p2s2_final\")\n",
    "    calculate_overall_score(query_2_hop[0:1], train_sampler, \"bfs\", label = \"p2s2_final\")\n",
    "    calculate_overall_score(query_3_hop[0:1], train_sampler, \"bfs\", label = \"p2s2_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37a156b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average unique elements: 113169.0\n"
     ]
    }
   ],
   "source": [
    "sums = 0 \n",
    "for a in all_subgraph['p2s2_final']:\n",
    "    subgraph = a['subgraph']\n",
    "    sums +=  len(np.unique(subgraph[:, [0, 2]].flatten()))\n",
    "\n",
    "avg =sums / len(all_subgraph)\n",
    "print(\"Average unique elements:\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055757e",
   "metadata": {},
   "source": [
    "### Sub-objective approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai error, retry: 1 validation error for SubobjectiveOutput\n",
      "  Invalid JSON: EOF while parsing a string at line 1 column 169 [type=json_invalid, input_value='{\"res\":[\"Identify the aw... the awards he has been', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
      "epoch 0\n",
      "epoch 1\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args, use_sub_objectives_a=True)\n",
    "\n",
    "query_1_hop_stat_a = []\n",
    "query_2_hop_stat_a = []\n",
    "query_3_hop_stat_a = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat_a.append(calculate_overall_score(query_1_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_2_hop_stat_a.append(calculate_overall_score(query_2_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_3_hop_stat_a.append(calculate_overall_score(query_3_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.2864\n",
      "  std_dev: 0.0002\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.0949\n",
      "  std_dev: 0.0367\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.0416\n",
      "  std_dev: 0.0312\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_statistics\n",
    "\n",
    "\n",
    "stats_1_hop_a = calculate_statistics(query_1_hop_stat_a)\n",
    "print_statistics(stats_1_hop_a, \"1-Hop Query\")\n",
    "stats_2_hop_a = calculate_statistics(query_2_hop_stat_a)\n",
    "print_statistics(stats_2_hop_a, \"2-Hop Query\")\n",
    "stats_3_hop_a = calculate_statistics(query_3_hop_stat_a)\n",
    "print_statistics(stats_3_hop_a, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c772d5",
   "metadata": {},
   "source": [
    "### Sub-objective approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d9d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> removing 1-hop links...\n",
      "==> done\n",
      "epoch 0\n",
      "==> removing 1-hop links...\n",
      "==> done\n",
      "epoch 1\n"
     ]
    }
   ],
   "source": [
    "train_sampler = ExpandSubgraph(args.n_ent, args.n_rel,\n",
    "                               train_graph_homo,train_graph,\n",
    "                               args=args, use_sub_objectives_b=True)\n",
    "\n",
    "query_1_hop_stat_b = []\n",
    "query_2_hop_stat_b = []\n",
    "query_3_hop_stat_b = []\n",
    "\n",
    "for _ in range(2):\n",
    "    loader.shuffle_train()\n",
    "    train_sampler.updateEdges(loader.train_graph)\n",
    "    random.shuffle(query_1_hop)\n",
    "    random.shuffle(query_2_hop)\n",
    "    random.shuffle(query_3_hop)\n",
    "    query_1_hop_stat_b.append(calculate_overall_score(query_1_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_2_hop_stat_b.append(calculate_overall_score(query_2_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    query_3_hop_stat_b.append(calculate_overall_score(query_3_hop[:50], train_sampler, label = \"subojective\"))\n",
    "    print(f\"epoch {_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44297ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Hop Query Statistics:\n",
      "  mean: 0.2926\n",
      "  std_dev: 0.0399\n",
      "2-Hop Query Statistics:\n",
      "  mean: 0.1010\n",
      "  std_dev: 0.0069\n",
      "3-Hop Query Statistics:\n",
      "  mean: 0.0475\n",
      "  std_dev: 0.0114\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_statistics\n",
    "\n",
    "\n",
    "stats_1_hop_b = calculate_statistics(query_1_hop_stat_b)\n",
    "print_statistics(stats_1_hop_b, \"1-Hop Query\")\n",
    "stats_2_hop_b = calculate_statistics(query_2_hop_stat_b)\n",
    "print_statistics(stats_2_hop_b, \"2-Hop Query\")\n",
    "stats_3_hop_b = calculate_statistics(query_3_hop_stat_b)\n",
    "print_statistics(stats_3_hop_b, \"3-Hop Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eadfaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad16362",
   "metadata": {},
   "source": [
    "### Analyzing the size of retrieved subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e049ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.31111111111111"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_size = 0\n",
    "for i in range(len(all_subgraph[\"p2s2\"])):\n",
    "    subgraph = all_subgraph[\"p2s2\"][i][\"subgraph\"]\n",
    "    # print(subgraph)\n",
    "    # print(subgraph[:,[0,2]].flatten())\n",
    "    number_of_nodes = np.unique(subgraph[:,[0,2]].flatten()).shape[0]\n",
    "    # print(number_of_nodes)\n",
    "    avg_size += number_of_nodes\n",
    "    # break\n",
    "avg_size /= len(all_subgraph[\"p2s2\"])\n",
    "avg_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85371e2",
   "metadata": {},
   "source": [
    "### output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47298b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "results = {\n",
    "    \"mine\":{\n",
    "        \"1-hop\": stats_1_hop,\n",
    "        \"2-hop\": stats_2_hop,\n",
    "        \"3-hop\": stats_3_hop\n",
    "    },\n",
    "    \"bfs\": {\n",
    "        \"1-hop\": stats_1_hop_bfs,\n",
    "        \"2-hop\": stats_2_hop_bfs,\n",
    "        \"3-hop\": stats_3_hop_bfs\n",
    "    },\n",
    "    \"subobjective\": {\n",
    "        \"1-hop\": stats_1_hop_a,\n",
    "        \"2-hop\": stats_2_hop_a,\n",
    "        \"3-hop\": stats_3_hop_a\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1149746",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/expand_subgraph/expand_subgraph_results_2_12.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518c7f2",
   "metadata": {},
   "source": [
    "## Show result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679cdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mine', 'bfs'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"results/expand_subgraph/expand_subgraph_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d71db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 3,\n",
       " 'mean': 0.44306987267385384,\n",
       " 'median': 0.451781993885975,\n",
       " 'std_dev': 0.012320799975220113,\n",
       " 'min': 0.42564563024961144,\n",
       " 'max': 0.451781993885975,\n",
       " '25th_percentile': 0.43871381206779325,\n",
       " '75th_percentile': 0.451781993885975}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['bfs']['3-hop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['sub_objective_a']={\n",
    "    '1-hop': {\n",
    "        'mean': 0.2864,\n",
    "        'std_dev': 0.0002,\n",
    "    },\n",
    "    '2-hop': {\n",
    "        'mean': 0.0949,\n",
    "        'std_dev': 0.0367,\n",
    "    },\n",
    "    '3-hop':{\n",
    "        'mean': 0.0416,\n",
    "        'std_dev': 0.0312\n",
    "    }\n",
    "}\n",
    "results['sub_objective_b']={\n",
    "    '1-hop': {\n",
    "        'mean': 0.2926,\n",
    "        'std_dev': 0.0399\n",
    "    },\n",
    "    '2-hop': {\n",
    "        'mean': 0.1010,\n",
    "        'std_dev': 0.0069\n",
    "    },\n",
    "    '3-hop':{\n",
    "        'mean': 0.0475,\n",
    "        'std_dev': 0.0114\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a79fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"data_path\": args.data_path,\n",
    "    \"seed\": args.seed,\n",
    "    \"k\": args.k,\n",
    "    \"depth\": args.depth,\n",
    "    \"gpu\": args.gpu,\n",
    "    \"fact_ratio\": args.fact_ratio,\n",
    "    \"val_num\": args.val_num,\n",
    "    \"epoch\": args.epoch,\n",
    "    \"layer\": args.layer,\n",
    "    \"batchsize\": args.batchsize,\n",
    "    \"cpu\": args.cpu,\n",
    "    \"weight\": args.weight,\n",
    "    \"add_manual_edges\": args.add_manual_edges,\n",
    "    \"remove_1hop_edges\": args.remove_1hop_edges,\n",
    "    \"only_eval\": args.only_eval,\n",
    "    \"not_shuffle_train\": args.not_shuffle_train,\n",
    "    \"device\": args.device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110b91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ida",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
